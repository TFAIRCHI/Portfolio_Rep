<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Linear Regression Charts</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="page-container">

  <h1>Linear Regression Model Analysis</h1>
    <section class="intro">
    <h2>About the Linear Regression Model</h2>
    <p>
      This analysis uses linear regression to predict housing sale prices based on key features
      such as square footage, property age, and other housing attributes. The primary objective of this model
      is to identify which variables most strongly influence home prices, providing valuable insights for
      developers, real estate professionals, and urban planners to make data-driven decisions.
    </p>
    <p>
      By analyzing training and test data distributions, fitting a multiple linear regression model, and
      evaluating residual plots, we assess both the performance of the model and its assumptions.
      The charts below illustrate these aspects of the analysis.
    </p>
  </section>

  <!--EDA-->
  <p>This preview shows the first 10 rows of the dataset, offering an overview of the key features and their formats before analysis.</p>
  <section class="MainCategory">
  <div class="card">
  <h3>Dataset Preview (First 10 rows)</h3>
  <iframe src="./Linear Regression/InitialDataSet.html" width="25%" height="400px" frameborder="0"></iframe>
  </div>


    <h2>Exploratory Data Analysis</h2>
    <p>Univariate distributions highlight potential data imbalances or outliers, while bivariate plots reveal linear relationships, correlations, and multicollinearity risks important for linear modeling.</p>
    <!-- Univariate Analysis -->
    <h3>Univariate Analysis: Assessing all variables independent from one another to observe any outliers that may drive skewness / bias</h3>    
  <div class="chart-row">
  <div class="card chart-card">
    <h3>Distribution of Age of Home</h3>
    <img src="Linear Regression/images/distribution_of_AgeOfHome.png" width="40%" height="500px" frameborder="0"></img>
  </div>

  <div class="card chart-card">
    <h3>Distribution of Backyard Space</h3>
    <img src="Linear Regression/images/distribution_of_BackyardSpace.png" width="40%" height="500px" frameborder="0"></img>
  </div>

  <div class="card chart-card">
    <h3>Distribution of Previous Sale Price</h3>
    <img src="Linear Regression/images/distribution_of_PreviousSalePrice.png" width="40%" height="500px" frameborder="0"></img>
  </div>

  <div class="card chart-card">
    <h3>Distribution of Price</h3>
    <img src="Linear Regression/images/distribution_of_Price.png" width="40%" height="500px" frameborder="0"></img>
  </div>
  <div class="card chart-card">
    <h3>Distribution of Property Tax Rate</h3>
    <img src="Linear Regression/images/distribution_of_PropertyTaxRate.png" width="40%" height="500px" frameborder="0"></img>
  </div> 
  <div class="card chart-card">
    <h3>Distribution of Square Footage</h3>
    <img src="Linear Regression/images/distribution_of_SquareFootage.png" width="40%" height="500px" frameborder="0"></img>
  </div>
  
    <h3>Bivariate Analysis: Assessing for Multicollinearity between independent variables and dependent variable</h3>

  <div class="chart-row">
  <div class="card chart-card">
    <h3>Price vs Age of Home</h3>
    <img src="Linear Regression/images/Bivariate_distribution_of_AgeOfHome.png" width="40%" height="500px" frameborder="0"></img>
  </div>

  <div class="card chart-card">
    <h3>Price vs Backyard Space</h3>
    <img src="Linear Regression/images/Bivariate_distribution_of_BackyardSpace.png" width="40%" height="500px" frameborder="0"></img>
  </div>

  <div class="card chart-card">
    <h3>Price vs Previous Sale Price</h3>
    <img src="Linear Regression/images/Bivariate_distribution_of_PreviousSalePrice.png" width="40%" height="500px" frameborder="0"></img>
  </div>

  <div class="card chart-card">
    <h3>Price vs Property Tax Rate</h3>
    <img src="Linear Regression/images/Bivariate_distribution_of_PropertyTaxRate.png" width="40%" height="500px" frameborder="0"></img>
  </div>
  <div class="card chart-card">
    <h3>Price vs Square Footage</h3>
    <img src="Linear Regression/images/Bivariate_distribution_of_SquareFootage.png" width="40%" height="500px" frameborder="0"></img>
  </div> 
  <div class="card chart-card">
    <h3></h3>
  </div>
 
  <section>
    <h3>Standardized Dataset of Predictor Values for PCA Analysis</h3>
    <p>PCA requires standardized (mean=0, std=1) data to avoid biasing component weights toward features with larger scales.</p>
    <p>Standardizing predictors ensures each feature contributes equally to PCA, preventing dominance by variables with larger numeric ranges.</p>
    <p>PCA identifies linear combinations of predictors that capture the most variance, simplifying the model and minimizing multicollinearity.</p>
    <div class="card">
        <iframe src="./Linear Regression/Standardized_Dataset.html" width="25%" height="400px" frameborder="0"></iframe>
    </div>
  
  
    <h3>PCA Component Matrix</h3>
    <div class="card">
    
            <iframe src="./Linear Regression/PCAComponent_Matrix.html" width="25%" height="400px" frameborder="0"></iframe>
    </div>
  
  
    <h2>Scree Plot with Principal Components</h2>
    <div class="card">
        <h4>Elbow Rule: The elbow rule suggests selecting the number of principal components at the point where the explained variance curve levels off, indicating diminishing returns; in this scree plot, that occurs around the second component.</h4>
        <h4>Kaiser Rule: The Kaiser rule recommends keeping principal components with eigenvalues greater than 1, which typically correspond to components explaining more variance than an individual standardized variable.</h4>
        <h4>Results: PCA 1 and PCA 2 are retained</h4>
        <img src="./Linear Regression/images/PCA_SCREEPLOT.png" width="40%" height="500px" frameborder="0"></img>
    </div>
  
    <!--<div>
        <h2>1 Initialize & Fit Model with PCA Retained Components</h2>
        <h2>2 Backwards Stepwise Elimination</h2>
        <h2>3 MSE & RMSE</h2>

    </div>-->

    <section class="category">
  <h2>Backward Stepwise Elimination - Optimization Technique Used</h2>
  <p>
    Backward stepwise elimination is a feature selection technique that starts with all candidate predictors in a model
    and iteratively removes the least significant variable — the one with the highest p-value exceeding a chosen threshold
    (often 0.05) — until only statistically significant predictors remain. This process refines the model by eliminating
    variables that don’t meaningfully contribute to explaining the target variable.
  </p>
  <p>
    Using backward elimination improves model interpretability, avoids overfitting by removing redundant predictors,
    and ensures that the final model includes only features with strong predictive power. In this analysis, backward
    elimination helped optimize the linear regression by retaining principal components that significantly explain housing prices.
  </p>
  <p>Results: PC1 and PC2 remain as the retained variables. OLS Below shows a P-Value < .05 indicating significance of fields</p>
</section>


<section class="category">

<h2>OLS Regression Summary - Optimized Model</h2>
  <p>Shows p-values, R², standard errors, confidence intervals — everything you need to assess significance and assumptions.</p>
  <p>The OLS regression summary confirms the retained principal components are statistically significant and summarizes overall model performance.</p>
  <div class="card">
        <iframe src="./Linear Regression/OLS_Reg_Summary.html" width="100%" height="600px" frameborder="0"></iframe>
      </div>
    </section>

  <h2>Metrics Baseball Cards Representing Model Accuracy</h2>
    <p>Clear comparison of train vs. test performance to assess overfitting.</p>
    <p>Quantifies predictive accuracy.</p>
    <p>Metrics cards provide a concise comparison of model error on training and test sets, helping evaluate predictive performance and generalization.</p>

<div class="metrics-cards-container">
  <div class="metrics-card">
    <h3>Train Metrics</h3>
    <div class="metric">
      <span class="label">MSE:</span>
      <span class="value">9,356,676,480.20</span>
    </div>
    <div class="metric">
      <span class="label">RMSE:</span>
      <span class="value">3.15</span>
    </div>
  </div>

  <div class="metrics-card">
    <h3>Test Metrics</h3>
    <div class="metric">
      <span class="label">MSE:</span>
      <span class="value">9,268,266,675.14</span>
    </div>
    <div class="metric">
      <span class="label">RMSE:</span>
      <span class="value">3.15</span>
    </div>
  </div>

</div>
      <h2>Overlay of Actual vs Predicted Sale Prices</h2>
        <p>Visually demonstrates how well the model predictions align with true sale prices.</p>
        <p>Highlights systematic errors or bias.</p>
        <p>This overlay plot allows quick visual inspection of prediction accuracy, revealing if the model systematically over- or under-predicts.</p>
    <div class="card">

      <img src="./Linear Regression/images/predicted_vs_actual.png" width="20%" height="40px" frameborder="15"></img>
    </div>
  <section class="category">
  <h2>Overall Findings</h2>
  <p>
    This linear regression analysis using principal component analysis (PCA) identified two principal components (PC1 and PC2) that captured the most significant variance in the standardized predictor variables. Through backward stepwise elimination, both retained components were confirmed as statistically significant predictors of housing sale prices.
  </p>
  <p>
    The final optimized model achieved comparable train and test RMSE values, suggesting minimal overfitting and strong generalization to unseen data. Visual inspection of the predicted vs. actual sale prices indicated good alignment, with no clear systematic under- or over-prediction patterns. Residual plots showed random scatter, supporting the assumptions of linear regression.
  </p>
  <p>
    Overall, the analysis demonstrates that dimensionality reduction via PCA combined with backward elimination can effectively reduce feature complexity while retaining predictive power, resulting in a robust and interpretable linear model for predicting housing sale prices.
  </p>
</section>

</section>

  </div>
</body>
</html>